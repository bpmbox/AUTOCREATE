#!/usr/bin/env python3
"""
AUTOCREATE WIKI RAGã‚·ã‚¹ãƒ†ãƒ 
- æ—¢å­˜ã®WIKIã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
- è‡ªç„¶è¨€èªã§ã®è³ªå•ãƒ»å›ç­”ã‚·ã‚¹ãƒ†ãƒ 
- ChromaDBã‚’ä½¿ç”¨ã—ãŸãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
"""

import os
import sys
import logging
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import chromadb
    from chromadb.config import Settings
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.vectorstores import Chroma
    from langchain_community.document_loaders import DirectoryLoader, TextLoader
    from langchain.schema import Document
    import gradio as gr
    import numpy as np
    import pandas as pd
    import markdown
    from bs4 import BeautifulSoup
    from dotenv import load_dotenv
except ImportError as e:
    print(f"âŒ ä¾å­˜é–¢ä¿‚ã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ“¦ ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("pip install -r requirements_wiki_rag.txt")
    sys.exit(1)

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/workspaces/AUTOCREATE/wiki_rag.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class WikiRAGSystem:
    """WIKI RAGã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, wiki_paths: List[str] = None, chroma_path: str = None):
        """åˆæœŸåŒ–"""
        self.wiki_paths = wiki_paths or [
            "/workspaces/AUTOCREATE/wikigit",
            "/workspaces/AUTOCREATE/AUTOCREATE.wiki",
            "/workspaces/AUTOCREATE/docs"
        ]
        self.chroma_path = chroma_path or "/workspaces/AUTOCREATE/chroma/wiki_rag"
        self.collection_name = "wiki_knowledge"
        
        # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", ".", " ", ""]
        )
        
        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆè»½é‡ã§èªè¨¼ä¸è¦ï¼‰
        self.embeddings = HuggingFaceEmbeddings(
            model_name="all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # ChromaDBè¨­å®š
        self.chroma_client = None
        self.vectorstore = None
        
        logger.info("ğŸš€ WIKI RAGã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
    
    def initialize_chroma(self):
        """ChromaDBåˆæœŸåŒ–"""
        try:
            # ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ
            os.makedirs(self.chroma_path, exist_ok=True)
            
            self.chroma_client = chromadb.PersistentClient(
                path=self.chroma_path,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆãƒ»å–å¾—
            try:
                collection = self.chroma_client.get_collection(name=self.collection_name)
                logger.info(f"âœ… æ—¢å­˜ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{self.collection_name}' ã‚’å–å¾—")
            except:
                collection = self.chroma_client.create_collection(
                    name=self.collection_name,
                    metadata={"description": "AUTOCREATE WIKI Knowledge Base"}
                )
                logger.info(f"ğŸ†• æ–°è¦ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{self.collection_name}' ã‚’ä½œæˆ")
            
            # LangChain Vectorstoreä½œæˆ
            self.vectorstore = Chroma(
                client=self.chroma_client,
                collection_name=self.collection_name,
                embedding_function=self.embeddings
            )
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ChromaDBåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def load_wiki_documents(self) -> List[Document]:
        """WIKIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿"""
        documents = []
        
        for wiki_path in self.wiki_paths:
            if not os.path.exists(wiki_path):
                logger.warning(f"âš ï¸ ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {wiki_path}")
                continue
            
            try:
                # Markdownãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                loader = DirectoryLoader(
                    wiki_path,
                    glob="**/*.md",
                    loader_cls=TextLoader,
                    loader_kwargs={'encoding': 'utf-8'},
                    recursive=True
                )
                
                wiki_docs = loader.load()
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
                for doc in wiki_docs:
                    doc.metadata.update({
                        'source_type': 'wiki',
                        'wiki_path': wiki_path,
                        'loaded_at': datetime.now().isoformat()
                    })
                
                documents.extend(wiki_docs)
                logger.info(f"ğŸ“„ {len(wiki_docs)}å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿: {wiki_path}")
                
            except Exception as e:
                logger.error(f"âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({wiki_path}): {e}")
        
        logger.info(f"ğŸ“š åˆè¨ˆ {len(documents)}å€‹ã®WIKIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿")
        return documents
    
    def process_documents(self, documents: List[Document]) -> List[Document]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‰å‡¦ç†"""
        processed_docs = []
        
        for doc in documents:
            try:
                # Markdownã‚’ HTMLã«å¤‰æ›ã—ã¦ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                html = markdown.markdown(doc.page_content)
                soup = BeautifulSoup(html, 'html.parser')
                clean_text = soup.get_text()
                
                # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
                chunks = self.text_splitter.split_text(clean_text)
                
                for i, chunk in enumerate(chunks):
                    if len(chunk.strip()) > 50:  # çŸ­ã™ãã‚‹ãƒãƒ£ãƒ³ã‚¯ã¯é™¤å¤–
                        chunk_doc = Document(
                            page_content=chunk,
                            metadata={
                                **doc.metadata,
                                'chunk_index': i,
                                'chunk_count': len(chunks)
                            }
                        )
                        processed_docs.append(chunk_doc)
                
            except Exception as e:
                logger.error(f"âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        
        logger.info(f"âœ‚ï¸ {len(processed_docs)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²")
        return processed_docs
    
    def build_knowledge_base(self, force_rebuild: bool = False):
        """ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰"""
        if not self.initialize_chroma():
            return False
        
        try:
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ç¢ºèª
            existing_count = self.vectorstore._collection.count()
            
            if existing_count > 0 and not force_rebuild:
                logger.info(f"ğŸ“– æ—¢å­˜ã®ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ ({existing_count}ä»¶)")
                return True
            
            if force_rebuild and existing_count > 0:
                logger.info("ğŸ”„ ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’å†æ§‹ç¯‰ä¸­...")
                self.chroma_client.delete_collection(self.collection_name)
                self.initialize_chroma()
            
            # WIKIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿ãƒ»å‡¦ç†
            logger.info("ğŸ“š WIKIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...")
            documents = self.load_wiki_documents()
            
            if not documents:
                logger.error("âŒ èª­ã¿è¾¼ã‚€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
                return False
            
            logger.info("âš™ï¸ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ä¸­...")
            processed_docs = self.process_documents(documents)
            
            if not processed_docs:
                logger.error("âŒ å‡¦ç†ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
                return False
            
            # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ»æ ¼ç´
            logger.info("ğŸ”„ ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ»æ ¼ç´ä¸­...")
            self.vectorstore.add_documents(processed_docs)
            
            final_count = self.vectorstore._collection.count()
            logger.info(f"âœ… ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰å®Œäº† ({final_count}ä»¶)")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def search_knowledge(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢"""
        if not self.vectorstore:
            logger.error("âŒ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return []
        
        try:
            # é¡ä¼¼åº¦æ¤œç´¢
            results = self.vectorstore.similarity_search_with_score(query, k=k)
            
            formatted_results = []
            for doc, score in results:
                formatted_results.append({
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'similarity_score': score,
                    'source': doc.metadata.get('source', 'unknown')
                })
            
            logger.info(f"ğŸ” '{query}' ã«å¯¾ã—ã¦ {len(formatted_results)}ä»¶ã®çµæœã‚’å–å¾—")
            return formatted_results
            
        except Exception as e:
            logger.error(f"âŒ ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def generate_answer(self, query: str, max_context_length: int = 2000) -> Dict[str, Any]:
        """å›ç­”ç”Ÿæˆ"""
        try:
            # é–¢é€£ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢
            search_results = self.search_knowledge(query, k=3)
            
            if not search_results:
                return {
                    'answer': 'ã“ã®ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚',
                    'sources': [],
                    'confidence': 0.0
                }
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
            context_parts = []
            sources = []
            
            for result in search_results:
                context_parts.append(result['content'])
                sources.append({
                    'source': result['source'],
                    'similarity': result['similarity_score']
                })
            
            context = '\n\n'.join(context_parts)[:max_context_length]
            
            # ç°¡å˜ãªå›ç­”ç”Ÿæˆï¼ˆå®Ÿéš›ã®LLMãªã—ã®å ´åˆï¼‰
            answer = self._generate_simple_answer(query, context, search_results)
            
            return {
                'answer': answer,
                'sources': sources,
                'confidence': search_results[0]['similarity_score'] if search_results else 0.0,
                'context_used': context[:500] + '...' if len(context) > 500 else context
            }
            
        except Exception as e:
            logger.error(f"âŒ å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'answer': f'å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}',
                'sources': [],
                'confidence': 0.0
            }
    
    def _generate_simple_answer(self, query: str, context: str, search_results: List[Dict]) -> str:
        """ç°¡å˜ãªå›ç­”ç”Ÿæˆï¼ˆLLMãªã—ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰"""
        # æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„çµæœã‚’åŸºã«å›ç­”ã‚’æ§‹ç¯‰
        if not search_results:
            return "é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        best_result = search_results[0]
        answer_parts = [
            f"**è³ªå•:** {query}",
            "",
            f"**å›ç­”:**",
            best_result['content'][:800] + ('...' if len(best_result['content']) > 800 else ''),
            "",
            f"**ã‚½ãƒ¼ã‚¹:** {best_result['metadata'].get('source', 'unknown')}",
            f"**ä¿¡é ¼åº¦:** {best_result['similarity_score']:.2f}"
        ]
        
        return '\n'.join(answer_parts)
    
    def get_statistics(self) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±å–å¾—"""
        if not self.vectorstore:
            return {'error': 'ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“'}
        
        try:
            count = self.vectorstore._collection.count()
            return {
                'total_documents': count,
                'collection_name': self.collection_name,
                'chroma_path': self.chroma_path,
                'wiki_paths': self.wiki_paths
            }
        except Exception as e:
            return {'error': str(e)}

def create_gradio_interface(rag_system: WikiRAGSystem):
    """Gradio Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ä½œæˆ"""
    
    def query_knowledge(question: str, max_results: int = 5):
        """ãƒŠãƒ¬ãƒƒã‚¸è³ªå•å‡¦ç†"""
        if not question.strip():
            return "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚", ""
        
        try:
            result = rag_system.generate_answer(question)
            
            # å›ç­”ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            answer = result['answer']
            
            # ã‚½ãƒ¼ã‚¹æƒ…å ±
            sources_info = "**é–¢é€£ã‚½ãƒ¼ã‚¹:**\n"
            for i, source in enumerate(result['sources'][:3], 1):
                sources_info += f"{i}. {source['source']} (é¡ä¼¼åº¦: {source['similarity']:.3f})\n"
            
            return answer, sources_info
            
        except Exception as e:
            logger.error(f"âŒ è³ªå•å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", ""
    
    def rebuild_knowledge_base():
        """ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹å†æ§‹ç¯‰"""
        try:
            success = rag_system.build_knowledge_base(force_rebuild=True)
            if success:
                stats = rag_system.get_statistics()
                return f"âœ… ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’å†æ§‹ç¯‰ã—ã¾ã—ãŸ\nğŸ“Š ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {stats.get('total_documents', 0)}"
            else:
                return "âŒ ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹å†æ§‹ç¯‰ã«å¤±æ•—ã—ã¾ã—ãŸ"
        except Exception as e:
            return f"âŒ ã‚¨ãƒ©ãƒ¼: {e}"
    
    def show_statistics():
        """çµ±è¨ˆæƒ…å ±è¡¨ç¤º"""
        stats = rag_system.get_statistics()
        if 'error' in stats:
            return f"âŒ ã‚¨ãƒ©ãƒ¼: {stats['error']}"
        
        return f"""
ğŸ“Š **WIKI RAGã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ**

ğŸ”¢ ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {stats['total_documents']}
ğŸ“ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å: {stats['collection_name']}
ğŸ’¾ Chromaãƒ‘ã‚¹: {stats['chroma_path']}
ğŸ“š WIKIãƒ‘ã‚¹: {', '.join(stats['wiki_paths'])}
        """
    
    # Gradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    with gr.Blocks(title="AUTOCREATE WIKI RAG System", theme=gr.themes.Soft()) as interface:
        gr.Markdown("""
        # ğŸ¤– AUTOCREATE WIKI RAG System
        
        æ—¢å­˜ã®WIKIãƒŠãƒ¬ãƒƒã‚¸ã‚’ä½¿ã£ãŸè‡ªç„¶è¨€èªQ&Aã‚·ã‚¹ãƒ†ãƒ 
        
        ## ä½¿ã„æ–¹
        1. è³ªå•ã‚’è‡ªç„¶è¨€èªã§å…¥åŠ›
        2. é–¢é€£ã™ã‚‹WIKIæƒ…å ±ã‚’è‡ªå‹•æ¤œç´¢
        3. é–¢é€£åº¦ã®é«˜ã„å›ç­”ã‚’å–å¾—
        """)
        
        with gr.Row():
            with gr.Column(scale=2):
                question_input = gr.Textbox(
                    label="è³ªå•",
                    placeholder="ä¾‹: Gradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ä½œæˆæ–¹æ³•ã¯ï¼Ÿ",
                    lines=2
                )
                
                max_results = gr.Slider(
                    minimum=1,
                    maximum=10,
                    value=5,
                    step=1,
                    label="æœ€å¤§çµæœæ•°"
                )
                
                query_btn = gr.Button("ğŸ” è³ªå•ã™ã‚‹", variant="primary")
                
            with gr.Column(scale=1):
                rebuild_btn = gr.Button("ğŸ”„ ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹å†æ§‹ç¯‰")
                stats_btn = gr.Button("ğŸ“Š çµ±è¨ˆæƒ…å ±")
        
        with gr.Row():
            with gr.Column():
                answer_output = gr.Markdown(label="å›ç­”")
                
            with gr.Column():
                sources_output = gr.Markdown(label="é–¢é€£ã‚½ãƒ¼ã‚¹")
        
        with gr.Row():
            system_output = gr.Markdown(label="ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
        
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©
        query_btn.click(
            query_knowledge,
            inputs=[question_input, max_results],
            outputs=[answer_output, sources_output]
        )
        
        rebuild_btn.click(
            rebuild_knowledge_base,
            outputs=[system_output]
        )
        
        stats_btn.click(
            show_statistics,
            outputs=[system_output]
        )
        
        # åˆæœŸçµ±è¨ˆè¡¨ç¤º
        interface.load(
            show_statistics,
            outputs=[system_output]
        )
    
    return interface

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ AUTOCREATE WIKI RAGã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹å§‹...")
    
    # RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    rag_system = WikiRAGSystem()
    
    # ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰
    logger.info("ğŸ“š ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ä¸­...")
    if not rag_system.build_knowledge_base():
        logger.error("âŒ ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹èµ·å‹•
    logger.info("ğŸŒ Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’èµ·å‹•ä¸­...")
    interface = create_gradio_interface(rag_system)
    
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )

if __name__ == "__main__":
    main()
