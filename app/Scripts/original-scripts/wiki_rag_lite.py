#!/usr/bin/env python3
"""
AUTOCREATE WIKI RAG ã‚·ã‚¹ãƒ†ãƒ ï¼ˆè»½é‡ç‰ˆï¼‰
- HuggingFaceèªè¨¼ä¸è¦
- ã‚·ãƒ³ãƒ—ãƒ«ãªTF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
- æ—¢å­˜WIKIã‹ã‚‰ã®è³ªå•å¿œç­”
"""

import os
import sys
import logging
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import re

# æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨è»½é‡ãªä¾å­˜é–¢ä¿‚ã®ã¿
try:
    import chromadb
    from chromadb.config import Settings
    import gradio as gr
    import numpy as np
    import pandas as pd
    import markdown
    from bs4 import BeautifulSoup
    from dotenv import load_dotenv
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
except ImportError as e:
    print(f"âŒ ä¾å­˜é–¢ä¿‚ã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ“¦ ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§è»½é‡ç‰ˆä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("pip install chromadb gradio numpy pandas markdown beautifulsoup4 python-dotenv scikit-learn")
    sys.exit(1)

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/workspaces/AUTOCREATE/wiki_rag_lite.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class WikiRAGLiteSystem:
    """WIKI RAG ã‚·ã‚¹ãƒ†ãƒ ï¼ˆè»½é‡ç‰ˆï¼‰"""
    
    def __init__(self, wiki_paths: List[str] = None, chroma_path: str = None):
        """åˆæœŸåŒ–"""
        self.wiki_paths = wiki_paths or [
            "/workspaces/AUTOCREATE/wikigit",
            "/workspaces/AUTOCREATE/AUTOCREATE.wiki",
            "/workspaces/AUTOCREATE/docs"
        ]
        self.chroma_path = chroma_path or "/workspaces/AUTOCREATE/chroma/wiki_rag_lite"
        
        # TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ï¼ˆæ—¥æœ¬èªå¯¾å¿œæ”¹å–„ï¼‰
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 3),
            token_pattern=r'[a-zA-Z0-9\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]+',
            min_df=1,
            max_df=0.98,
            lowercase=True,
            analyzer='word'
        )
        
        # ãƒ‡ãƒ¼ã‚¿æ ¼ç´ç”¨
        self.documents = []
        self.document_vectors = None
        self.built = False
        
        logger.info("ğŸš€ WIKI RAG ã‚·ã‚¹ãƒ†ãƒ ï¼ˆè»½é‡ç‰ˆï¼‰ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
    
    def load_wiki_documents(self) -> List[Dict[str, Any]]:
        """WIKIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿"""
        documents = []
        
        for wiki_path in self.wiki_paths:
            if not os.path.exists(wiki_path):
                logger.warning(f"âš ï¸ ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {wiki_path}")
                continue
            
            try:
                for root, dirs, files in os.walk(wiki_path):
                    for filename in files:
                        if filename.endswith('.md'):
                            file_path = os.path.join(root, filename)
                            
                            with open(file_path, 'r', encoding='utf-8') as f:
                                content = f.read()
                            
                            # Markdownã‚’ HTMLã«å¤‰æ›ã—ã¦ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                            html = markdown.markdown(content)
                            soup = BeautifulSoup(html, 'html.parser')
                            clean_text = soup.get_text()
                            
                            # ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
                            processed_text = self._preprocess_text(clean_text)
                            
                            if len(processed_text.strip()) > 50:  # çŸ­ã™ãã‚‹æ–‡æ›¸ã¯é™¤å¤–
                                doc = {
                                    'content': processed_text,
                                    'original_content': content,
                                    'source': filename,
                                    'path': file_path,
                                    'source_type': 'wiki',
                                    'wiki_path': wiki_path,
                                    'loaded_at': datetime.now().isoformat()
                                }
                                documents.append(doc)
                
                logger.info(f"ğŸ“„ {len([d for d in documents if d['wiki_path'] == wiki_path])}å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿: {wiki_path}")
                
            except Exception as e:
                logger.error(f"âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({wiki_path}): {e}")
        
        logger.info(f"ğŸ“š åˆè¨ˆ {len(documents)}å€‹ã®WIKIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿")
        return documents
    
    def _preprocess_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†"""
        # æ”¹è¡Œãƒ»ç©ºç™½ã®æ­£è¦åŒ–
        text = re.sub(r'\n+', '\n', text)
        text = re.sub(r'\s+', ' ', text)
        
        # ç‰¹æ®Šæ–‡å­—ã®é™¤å»ï¼ˆåŸºæœ¬çš„ãªæ–‡å­—ã®ã¿æ®‹ã™ï¼‰
        text = re.sub(r'[^\w\s\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAFã€‚ã€ï¼ï¼Ÿ]', ' ', text)
        
        return text.strip()
    
    def build_knowledge_base(self, force_rebuild: bool = False):
        """ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰"""
        if self.built and not force_rebuild:
            logger.info("âœ… æ—¢å­˜ã®ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨")
            return True
        
        try:
            # WIKIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿
            logger.info("ğŸ“š WIKIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...")
            self.documents = self.load_wiki_documents()
            
            if not self.documents:
                logger.error("âŒ èª­ã¿è¾¼ã‚€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
                return False
            
            # TF-IDF ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            logger.info("ğŸ”„ TF-IDF ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­...")
            document_texts = [doc['content'] for doc in self.documents]
            self.document_vectors = self.vectorizer.fit_transform(document_texts)
            
            self.built = True
            logger.info(f"âœ… ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰å®Œäº† ({len(self.documents)}ä»¶)")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def search_knowledge(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢"""
        if not self.built:
            logger.error("âŒ ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return []
        
        try:
            # ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            query_processed = self._preprocess_text(query)
            query_vector = self.vectorizer.transform([query_processed])
            
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
            similarities = cosine_similarity(query_vector, self.document_vectors).flatten()
            
            # ä¸Šä½kä»¶ã®çµæœã‚’å–å¾—
            top_indices = np.argsort(similarities)[::-1][:k]
            
            results = []
            for idx in top_indices:
                if similarities[idx] > 0.01:  # æœ€ä½é¡ä¼¼åº¦ã—ãã„å€¤ã‚’ä¸‹ã’ã‚‹
                    results.append({
                        'content': self.documents[idx]['content'],
                        'metadata': {
                            'source': self.documents[idx]['source'],
                            'path': self.documents[idx]['path'],
                            'source_type': self.documents[idx]['source_type']
                        },
                        'similarity_score': float(similarities[idx]),
                        'source': self.documents[idx]['source']
                    })
            
            logger.info(f"ğŸ” '{query}' ã«å¯¾ã—ã¦ {len(results)}ä»¶ã®çµæœã‚’å–å¾—")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def generate_answer(self, query: str, max_context_length: int = 2000) -> Dict[str, Any]:
        """å›ç­”ç”Ÿæˆ"""
        try:
            # é–¢é€£ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢
            search_results = self.search_knowledge(query, k=3)
            
            if not search_results:
                return {
                    'answer': 'ã“ã®ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚',
                    'sources': [],
                    'confidence': 0.0
                }
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
            context_parts = []
            sources = []
            
            for result in search_results:
                context_parts.append(result['content'])
                sources.append({
                    'source': result['source'],
                    'similarity': result['similarity_score']
                })
            
            context = '\n\n'.join(context_parts)[:max_context_length]
            
            # ç°¡å˜ãªå›ç­”ç”Ÿæˆ
            answer = self._generate_simple_answer(query, context, search_results)
            
            return {
                'answer': answer,
                'sources': sources,
                'confidence': search_results[0]['similarity_score'] if search_results else 0.0,
                'context_used': context[:500] + '...' if len(context) > 500 else context
            }
            
        except Exception as e:
            logger.error(f"âŒ å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'answer': f'å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}',
                'sources': [],
                'confidence': 0.0
            }
    
    def _generate_simple_answer(self, query: str, context: str, search_results: List[Dict]) -> str:
        """ç°¡å˜ãªå›ç­”ç”Ÿæˆ"""
        if not search_results:
            return "é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        best_result = search_results[0]
        answer_parts = [
            f"**è³ªå•:** {query}",
            "",
            f"**å›ç­”:**",
            best_result['content'][:800] + ('...' if len(best_result['content']) > 800 else ''),
            "",
            f"**ã‚½ãƒ¼ã‚¹:** {best_result['metadata'].get('source', 'unknown')}",
            f"**ä¿¡é ¼åº¦:** {best_result['similarity_score']:.2f}"
        ]
        
        return '\n'.join(answer_parts)
    
    def get_statistics(self) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±å–å¾—"""
        if not self.built:
            return {'error': 'ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“'}
        
        return {
            'total_documents': len(self.documents),
            'vectorizer_features': self.vectorizer.max_features,
            'wiki_paths': self.wiki_paths,
            'built': self.built
        }

def create_gradio_interface(rag_system: WikiRAGLiteSystem):
    """Gradio Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ä½œæˆ"""
    
    def query_knowledge(question: str, max_results: int = 5):
        """ãƒŠãƒ¬ãƒƒã‚¸è³ªå•å‡¦ç†"""
        if not question.strip():
            return "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚", ""
        
        try:
            result = rag_system.generate_answer(question)
            
            # å›ç­”ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            answer = result['answer']
            
            # ã‚½ãƒ¼ã‚¹æƒ…å ±
            sources_info = "**é–¢é€£ã‚½ãƒ¼ã‚¹:**\n"
            for i, source in enumerate(result['sources'][:3], 1):
                sources_info += f"{i}. {source['source']} (é¡ä¼¼åº¦: {source['similarity']:.3f})\n"
            
            return answer, sources_info
            
        except Exception as e:
            logger.error(f"âŒ è³ªå•å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", ""
    
    def rebuild_knowledge_base():
        """ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹å†æ§‹ç¯‰"""
        try:
            success = rag_system.build_knowledge_base(force_rebuild=True)
            if success:
                stats = rag_system.get_statistics()
                return f"âœ… ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’å†æ§‹ç¯‰ã—ã¾ã—ãŸ\nğŸ“Š ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {stats.get('total_documents', 0)}"
            else:
                return "âŒ ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹å†æ§‹ç¯‰ã«å¤±æ•—ã—ã¾ã—ãŸ"
        except Exception as e:
            return f"âŒ ã‚¨ãƒ©ãƒ¼: {e}"
    
    def show_statistics():
        """çµ±è¨ˆæƒ…å ±è¡¨ç¤º"""
        stats = rag_system.get_statistics()
        if 'error' in stats:
            return f"âŒ ã‚¨ãƒ©ãƒ¼: {stats['error']}"
        
        return f"""
ğŸ“Š **WIKI RAG ã‚·ã‚¹ãƒ†ãƒ ï¼ˆè»½é‡ç‰ˆï¼‰çµ±è¨ˆ**

ğŸ”¢ ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {stats['total_documents']}
ğŸ”§ ãƒ™ã‚¯ãƒˆãƒ«ç‰¹å¾´æ•°: {stats['vectorizer_features']}
ğŸ“š WIKIãƒ‘ã‚¹: {', '.join(stats['wiki_paths'])}
âœ… æ§‹ç¯‰æ¸ˆã¿: {stats['built']}
        """
    
    # Gradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    with gr.Blocks(title="AUTOCREATE WIKI RAG Lite", theme=gr.themes.Soft()) as interface:
        gr.Markdown("""
        # ğŸ¤– AUTOCREATE WIKI RAG System (è»½é‡ç‰ˆ)
        
        **AIç¤¾é•·Ã—ç„¡è·CTOä½“åˆ¶**ã«ã‚ˆã‚‹é©æ–°çš„ãªãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ï¼ˆè»½é‡ç‰ˆï¼‰
        
        TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿæ¤œç´¢ãƒ»å›ç­”ã‚·ã‚¹ãƒ†ãƒ 
        """)
        
        with gr.Row():
            with gr.Column(scale=2):
                question_input = gr.Textbox(
                    label="è³ªå•",
                    placeholder="ä¾‹: Gradioã®ä½¿ã„æ–¹ã‚’æ•™ãˆã¦ãã ã•ã„",
                    lines=2
                )
                
                max_results = gr.Slider(
                    minimum=1,
                    maximum=10,
                    value=5,
                    step=1,
                    label="æœ€å¤§æ¤œç´¢çµæœæ•°"
                )
                
                submit_btn = gr.Button("ğŸ” è³ªå•ã™ã‚‹", variant="primary")
                
            with gr.Column(scale=1):
                rebuild_btn = gr.Button("ğŸ”„ ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹å†æ§‹ç¯‰")
                stats_btn = gr.Button("ğŸ“Š çµ±è¨ˆæƒ…å ±")
        
        with gr.Row():
            with gr.Column():
                answer_output = gr.Markdown(label="å›ç­”")
                
            with gr.Column():
                sources_output = gr.Markdown(label="é–¢é€£ã‚½ãƒ¼ã‚¹")
        
        with gr.Row():
            system_output = gr.Markdown(label="ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
        
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©
        submit_btn.click(
            query_knowledge,
            inputs=[question_input, max_results],
            outputs=[answer_output, sources_output]
        )
        
        rebuild_btn.click(
            rebuild_knowledge_base,
            outputs=[system_output]
        )
        
        stats_btn.click(
            show_statistics,
            outputs=[system_output]
        )
        
        # åˆæœŸçµ±è¨ˆè¡¨ç¤º
        interface.load(
            show_statistics,
            outputs=[system_output]
        )
    
    return interface

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ AUTOCREATE WIKI RAG ã‚·ã‚¹ãƒ†ãƒ ï¼ˆè»½é‡ç‰ˆï¼‰ã‚’é–‹å§‹...")
    
    # RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    rag_system = WikiRAGLiteSystem()
    
    # ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰
    logger.info("ğŸ“š ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ä¸­...")
    if not rag_system.build_knowledge_base():
        logger.error("âŒ ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹èµ·å‹•
    logger.info("ğŸŒ Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’èµ·å‹•ä¸­...")
    interface = create_gradio_interface(rag_system)
    
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )

if __name__ == "__main__":
    main()
