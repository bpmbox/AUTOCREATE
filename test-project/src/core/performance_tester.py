"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆæ©Ÿèƒ½ - Performance Tester Module

å®Ÿè¡Œæ™‚é–“æ¸¬å®šã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–ã€è² è·ãƒ†ã‚¹ãƒˆã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¤œå‡ºã‚’è¡Œã†
é«˜åº¦ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
"""

import time
import psutil
import threading
import concurrent.futures
import memory_profiler
from typing import Dict, List, Any, Optional, Callable, Tuple
from dataclasses import dataclass, field
from contextlib import contextmanager
import matplotlib.pyplot as plt
import json
from pathlib import Path
import statistics

@dataclass
class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šçµæœ"""
    test_name: str
    execution_time: float
    memory_usage: float
    cpu_usage: float
    peak_memory: float
    iterations: int
    throughput: float
    timestamp: float = field(default_factory=time.time)
    metadata: Dict[str, Any] = field(default_factory=dict)

class PerformanceTester:
    """
    é«˜åº¦ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
    
    Features:
    - å®Ÿè¡Œæ™‚é–“æ¸¬å®š
    - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–
    - CPUä½¿ç”¨ç‡æ¸¬å®š
    - è² è·ãƒ†ã‚¹ãƒˆ (Load Testing)
    - ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ (Stress Testing)
    - çµæœã®å¯è¦–åŒ–ãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    """
    
    def __init__(self, output_dir: str = "reports/performance"):
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ã‚¿ãƒ¼åˆæœŸåŒ–
        
        Args:
            output_dir: ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¸¬å®šçµæœä¿å­˜
        self.metrics: List[PerformanceMetrics] = []
        self.active_measurements: Dict[str, Dict] = {}
          # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—
        import platform
        self.system_info = {
            'cpu_count': psutil.cpu_count(),
            'memory_total': psutil.virtual_memory().total,
            'platform': platform.system(),
        }
    
    @contextmanager
    def measure_performance(self, test_name: str, iterations: int = 1):
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        
        Args:
            test_name: ãƒ†ã‚¹ãƒˆå
            iterations: ç¹°ã‚Šè¿”ã—å›æ•°
        """
        # æ¸¬å®šé–‹å§‹
        start_time = time.time()
        start_memory = psutil.virtual_memory().used
        process = psutil.Process()
        
        # CPUä½¿ç”¨ç‡æ¸¬å®šç”¨ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
        cpu_before = process.cpu_percent()
        
        measurement_data = {
            'start_time': start_time,
            'start_memory': start_memory,
            'peak_memory': start_memory,
            'cpu_samples': []
        }
        
        self.active_measurements[test_name] = measurement_data
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã®ç›£è¦–é–‹å§‹
        monitor_thread = threading.Thread(
            target=self._monitor_resources, 
            args=(test_name,), 
            daemon=True
        )
        monitor_thread.start()
        
        try:
            yield self
        finally:
            # æ¸¬å®šçµ‚äº†
            end_time = time.time()
            end_memory = psutil.virtual_memory().used
            cpu_after = process.cpu_percent()
            
            # çµæœè¨ˆç®—
            execution_time = end_time - start_time
            memory_usage = end_memory - start_memory
            cpu_usage = cpu_after - cpu_before
            peak_memory = measurement_data['peak_memory'] - start_memory
            throughput = iterations / execution_time if execution_time > 0 else 0
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
            metrics = PerformanceMetrics(
                test_name=test_name,
                execution_time=execution_time,
                memory_usage=memory_usage,
                cpu_usage=cpu_usage,
                peak_memory=peak_memory,
                iterations=iterations,
                throughput=throughput
            )
            
            self.metrics.append(metrics)
            
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–æ¸¬å®šã‹ã‚‰å‰Šé™¤
            self.active_measurements.pop(test_name, None)
    
    def _monitor_resources(self, test_name: str):
        """
        ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ç›£è¦–
        
        Args:
            test_name: ç›£è¦–å¯¾è±¡ã®ãƒ†ã‚¹ãƒˆå
        """
        measurement = self.active_measurements.get(test_name)
        if not measurement:
            return
        
        while test_name in self.active_measurements:
            try:
                current_memory = psutil.virtual_memory().used
                measurement['peak_memory'] = max(
                    measurement['peak_memory'], 
                    current_memory
                )
                
                # CPUä½¿ç”¨ç‡ã‚µãƒ³ãƒ—ãƒ«å–å¾—
                cpu_percent = psutil.cpu_percent(interval=0.1)
                measurement['cpu_samples'].append(cpu_percent)
                
                time.sleep(0.1)  # 100msé–“éš”ã§ç›£è¦–
                
            except Exception:
                break  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç›£è¦–çµ‚äº†
    
    def benchmark_function(self, 
                          func: Callable, 
                          iterations: int = 100,
                          warmup: int = 10,
                          test_name: Optional[str] = None) -> PerformanceMetrics:
        """
        é–¢æ•°ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ
        
        Args:
            func: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å¯¾è±¡ã®é–¢æ•°
            iterations: å®Ÿè¡Œå›æ•°
            warmup: ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å›æ•°
            test_name: ãƒ†ã‚¹ãƒˆå
            
        Returns:
            ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šçµæœ
        """
        if test_name is None:
            test_name = getattr(func, '__name__', 'benchmark_test')
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
        for _ in range(warmup):
            func()
        
        # æœ¬æ¸¬å®š
        with self.measure_performance(test_name, iterations):
            for _ in range(iterations):
                func()
        
        return self.metrics[-1]
    
    def load_test(self, 
                  func: Callable, 
                  concurrent_users: int = 10,
                  duration: float = 30.0,
                  ramp_up: float = 5.0,
                  test_name: Optional[str] = None) -> Dict[str, Any]:
        """
        è² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        
        Args:
            func: ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®é–¢æ•°
            concurrent_users: åŒæ™‚å®Ÿè¡Œãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°
            duration: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“ï¼ˆç§’ï¼‰
            ramp_up: ãƒ©ãƒ³ãƒ—ã‚¢ãƒƒãƒ—æ™‚é–“ï¼ˆç§’ï¼‰
            test_name: ãƒ†ã‚¹ãƒˆå
            
        Returns:
            è² è·ãƒ†ã‚¹ãƒˆçµæœ
        """
        if test_name is None:
            test_name = f"load_test_{getattr(func, '__name__', 'function')}"
        
        results = {
            'test_name': test_name,
            'concurrent_users': concurrent_users,
            'duration': duration,
            'start_time': time.time(),
            'executions': [],
            'errors': [],
            'statistics': {}
        }
        
        def user_simulation():
            """å˜ä¸€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
            user_results = []
            end_time = time.time() + duration
            
            while time.time() < end_time:
                try:
                    start = time.time()
                    func()
                    execution_time = time.time() - start
                    user_results.append({
                        'timestamp': start,
                        'duration': execution_time,
                        'success': True
                    })
                except Exception as e:
                    user_results.append({
                        'timestamp': time.time(),
                        'duration': 0,
                        'success': False,
                        'error': str(e)
                    })
                
                # å°‘ã—å¾…æ©Ÿï¼ˆãƒªã‚¢ãƒ«ãªãƒ¦ãƒ¼ã‚¶ãƒ¼å‹•ä½œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
                time.sleep(0.01)
            
            return user_results
        
        # ä¸¦åˆ—å®Ÿè¡Œã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            # ãƒ©ãƒ³ãƒ—ã‚¢ãƒƒãƒ—æœŸé–“ã§æ®µéšçš„ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼è¿½åŠ 
            futures = []
            for i in range(concurrent_users):
                if i > 0:
                    time.sleep(ramp_up / concurrent_users)
                
                future = executor.submit(user_simulation)
                futures.append(future)
            
            # å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®çµæœã‚’åé›†
            for future in concurrent.futures.as_completed(futures):
                try:
                    user_results = future.result()
                    results['executions'].extend(user_results)
                except Exception as e:
                    results['errors'].append(str(e))
        
        # çµ±è¨ˆè¨ˆç®—
        successful_executions = [e for e in results['executions'] if e['success']]
        if successful_executions:
            durations = [e['duration'] for e in successful_executions]
            results['statistics'] = {
                'total_executions': len(results['executions']),
                'successful_executions': len(successful_executions),
                'failed_executions': len(results['executions']) - len(successful_executions),
                'success_rate': len(successful_executions) / len(results['executions']) * 100,
                'average_response_time': statistics.mean(durations),
                'median_response_time': statistics.median(durations),
                'min_response_time': min(durations),
                'max_response_time': max(durations),
                'throughput': len(successful_executions) / duration,
                'total_duration': time.time() - results['start_time']
            }
        
        return results
    
    def stress_test(self, 
                   func: Callable,
                   max_users: int = 100,
                   step_size: int = 10,
                   step_duration: float = 30.0,
                   test_name: Optional[str] = None) -> Dict[str, Any]:
        """
        ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        
        Args:
            func: ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®é–¢æ•°
            max_users: æœ€å¤§ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°
            step_size: ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å¢—åŠ æ•°
            step_duration: å„ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œæ™‚é–“
            test_name: ãƒ†ã‚¹ãƒˆå
            
        Returns:
            ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆçµæœ
        """
        if test_name is None:
            test_name = f"stress_test_{getattr(func, '__name__', 'function')}"
        
        stress_results = {
            'test_name': test_name,
            'max_users': max_users,
            'step_size': step_size,
            'step_duration': step_duration,
            'steps': [],
            'breaking_point': None
        }
        
        # æ®µéšçš„ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ã‚’å¢—åŠ 
        for users in range(step_size, max_users + 1, step_size):
            print(f"ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­: {users}ãƒ¦ãƒ¼ã‚¶ãƒ¼")
            
            # è² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            load_result = self.load_test(
                func=func,
                concurrent_users=users,
                duration=step_duration,
                ramp_up=step_duration * 0.2,  # 20%ã‚’ãƒ©ãƒ³ãƒ—ã‚¢ãƒƒãƒ—ã«ä½¿ç”¨
                test_name=f"{test_name}_step_{users}"
            )
            
            step_result = {
                'users': users,
                'statistics': load_result['statistics'],
                'system_load': {
                    'cpu_percent': psutil.cpu_percent(),
                    'memory_percent': psutil.virtual_memory().percent,
                    'load_average': getattr(psutil, 'getloadavg', lambda: [0, 0, 0])()
                }
            }
            
            stress_results['steps'].append(step_result)
            
            # ç ´ç¶»ç‚¹ã®æ¤œå‡º
            stats = load_result['statistics']
            if (stats['success_rate'] < 95 or  # æˆåŠŸç‡95%æœªæº€
                stats['average_response_time'] > 5.0 or  # å¹³å‡å¿œç­”æ™‚é–“5ç§’è¶…
                psutil.cpu_percent() > 90):  # CPUä½¿ç”¨ç‡90%è¶…
                
                stress_results['breaking_point'] = {
                    'users': users,
                    'reason': 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–æ¤œå‡º',
                    'metrics': step_result
                }
                print(f"ç ´ç¶»ç‚¹æ¤œå‡º: {users}ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–")
                break
        
        return stress_results
    
    def generate_performance_report(self, filename: str = "performance_report.json"):
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        
        Args:
            filename: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
        """
        report_data = {
            'timestamp': time.time(),
            'system_info': self.system_info,
            'test_summary': {
                'total_tests': len(self.metrics),
                'total_execution_time': sum(m.execution_time for m in self.metrics),
                'average_execution_time': statistics.mean([m.execution_time for m in self.metrics]) if self.metrics else 0,
                'peak_memory_usage': max([m.peak_memory for m in self.metrics]) if self.metrics else 0,
                'average_throughput': statistics.mean([m.throughput for m in self.metrics]) if self.metrics else 0
            },
            'metrics': [
                {
                    'test_name': m.test_name,
                    'execution_time': m.execution_time,
                    'memory_usage': m.memory_usage,
                    'cpu_usage': m.cpu_usage,
                    'peak_memory': m.peak_memory,
                    'iterations': m.iterations,
                    'throughput': m.throughput,
                    'timestamp': m.timestamp,
                    'metadata': m.metadata
                }
                for m in self.metrics
            ]
        }
        
        output_path = self.output_dir / filename
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {output_path}")
        return output_path
    
    def visualize_performance(self, save_plots: bool = True):
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµæœã®å¯è¦–åŒ–
        
        Args:
            save_plots: ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
        """
        if not self.metrics:
            print("å¯è¦–åŒ–ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # å®Ÿè¡Œæ™‚é–“ã®å¯è¦–åŒ–
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœ', fontsize=16)
        
        # å®Ÿè¡Œæ™‚é–“
        test_names = [m.test_name for m in self.metrics]
        execution_times = [m.execution_time for m in self.metrics]
        
        axes[0, 0].bar(test_names, execution_times)
        axes[0, 0].set_title('å®Ÿè¡Œæ™‚é–“ (ç§’)')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        memory_usage = [m.memory_usage / (1024 * 1024) for m in self.metrics]  # MBå¤‰æ›
        axes[0, 1].bar(test_names, memory_usage)
        axes[0, 1].set_title('ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ
        throughput = [m.throughput for m in self.metrics]
        axes[1, 0].bar(test_names, throughput)
        axes[1, 0].set_title('ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ (ops/sec)')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # å®Ÿè¡Œæ™‚é–“åˆ†å¸ƒ
        axes[1, 1].hist(execution_times, bins=10, alpha=0.7)
        axes[1, 1].set_title('å®Ÿè¡Œæ™‚é–“åˆ†å¸ƒ')
        axes[1, 1].set_xlabel('å®Ÿè¡Œæ™‚é–“ (ç§’)')
        axes[1, 1].set_ylabel('é »åº¦')
        
        plt.tight_layout()
        
        if save_plots:
            plot_path = self.output_dir / "performance_charts.png"
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚°ãƒ©ãƒ•ä¿å­˜: {plot_path}")
        
        plt.show()
    
    def print_summary(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        if not self.metrics:
            print("è¡¨ç¤ºã™ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        print("\n" + "="*70)
        print("ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
        print("="*70)
        
        for metric in self.metrics:
            print(f"\nğŸ“Š ãƒ†ã‚¹ãƒˆ: {metric.test_name}")
            print(f"   â±ï¸  å®Ÿè¡Œæ™‚é–“: {metric.execution_time:.4f}ç§’")
            print(f"   ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {metric.memory_usage / (1024*1024):.2f}MB")
            print(f"   ğŸ–¥ï¸  CPUä½¿ç”¨ç‡: {metric.cpu_usage:.2f}%")
            print(f"   ğŸ“ˆ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {metric.throughput:.2f} ops/sec")
            print(f"   ğŸ”„ ç¹°ã‚Šè¿”ã—å›æ•°: {metric.iterations}")
        
        # çµ±è¨ˆæƒ…å ±
        if len(self.metrics) > 1:
            execution_times = [m.execution_time for m in self.metrics]
            memory_usages = [m.memory_usage for m in self.metrics]
            throughputs = [m.throughput for m in self.metrics]
            
            print(f"\nğŸ“ˆ çµ±è¨ˆæƒ…å ±:")
            print(f"   å¹³å‡å®Ÿè¡Œæ™‚é–“: {statistics.mean(execution_times):.4f}ç§’")
            print(f"   æœ€é€Ÿãƒ†ã‚¹ãƒˆ: {min(execution_times):.4f}ç§’")
            print(f"   æœ€é…ãƒ†ã‚¹ãƒˆ: {max(execution_times):.4f}ç§’")
            print(f"   å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {statistics.mean(memory_usages) / (1024*1024):.2f}MB")
            print(f"   å¹³å‡ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {statistics.mean(throughputs):.2f} ops/sec")
        
        print("="*70)

# ä½¿ç”¨ä¾‹ã¨ãƒ‡ãƒ¢
if __name__ == "__main__":
    
    def sample_cpu_intensive():
        """CPUé›†ç´„çš„ãªå‡¦ç†ã®ã‚µãƒ³ãƒ—ãƒ«"""
        result = 0
        for i in range(100000):
            result += i ** 2
        return result
    
    def sample_memory_intensive():
        """ãƒ¡ãƒ¢ãƒªé›†ç´„çš„ãªå‡¦ç†ã®ã‚µãƒ³ãƒ—ãƒ«"""
        data = []
        for i in range(10000):
            data.append([j for j in range(100)])
        return len(data)
    
    def sample_io_operation():
        """I/Oæ“ä½œã®ã‚µãƒ³ãƒ—ãƒ«"""
        import tempfile
        import os
        
        with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:
            for i in range(1000):
                f.write(f"è¡Œ{i}: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿\n")
            temp_path = f.name
        
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        with open(temp_path, 'r') as f:
            lines = f.readlines()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        os.unlink(temp_path)
        return len(lines)
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ã‚¿ãƒ¼ã®ä½¿ç”¨ä¾‹
    tester = PerformanceTester()
    
    # å„ãƒ†ã‚¹ãƒˆã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    print("ğŸ§ª ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
    
    tester.benchmark_function(sample_cpu_intensive, iterations=50, test_name="CPUé›†ç´„å‡¦ç†")
    tester.benchmark_function(sample_memory_intensive, iterations=20, test_name="ãƒ¡ãƒ¢ãƒªé›†ç´„å‡¦ç†")
    tester.benchmark_function(sample_io_operation, iterations=30, test_name="I/Oæ“ä½œ")
    
    # çµæœè¡¨ç¤º
    tester.print_summary()
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    tester.generate_performance_report()
    
    # å¯è¦–åŒ–
    tester.visualize_performance()
    
    # è² è·ãƒ†ã‚¹ãƒˆã®ãƒ‡ãƒ¢
    print("\nğŸ”¥ è² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
    load_result = tester.load_test(
        func=sample_cpu_intensive,
        concurrent_users=5,
        duration=10.0,
        test_name="CPUè² è·ãƒ†ã‚¹ãƒˆ"
    )
    
    print(f"è² è·ãƒ†ã‚¹ãƒˆçµæœ:")
    print(f"  ç·å®Ÿè¡Œæ•°: {load_result['statistics']['total_executions']}")
    print(f"  æˆåŠŸç‡: {load_result['statistics']['success_rate']:.1f}%")
    print(f"  å¹³å‡å¿œç­”æ™‚é–“: {load_result['statistics']['average_response_time']:.4f}ç§’")
    print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {load_result['statistics']['throughput']:.2f} ops/sec")
